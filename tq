[1mdiff --git a/algorithms/engine/ffm_fedavg_depthffm_fim.py b/algorithms/engine/ffm_fedavg_depthffm_fim.py[m
[1mindex 7680343..9bbbee8 100755[m
[1m--- a/algorithms/engine/ffm_fedavg_depthffm_fim.py[m
[1m+++ b/algorithms/engine/ffm_fedavg_depthffm_fim.py[m
[36m@@ -339,6 +339,20 @@[m [mdef ffm_fedavg_depthffm_fim(args):[m
             args.block_ids_list = saved_block_ids_list[m
             args.saved_rank_list = saved_rank_list[m
 [m
[32m+[m[32m        if args.warm_start:[m
[32m+[m[32m            print('#####warm start round#####')[m
[32m+[m[32m            # warm start for our method[m
[32m+[m[32m            if t<20:[m
[32m+[m[32m                args.train_a = True[m
[32m+[m[32m                args.train_b = True[m
[32m+[m[32m            else:[m
[32m+[m[32m                if (t%2)==0:[m
[32m+[m[32m                    args.train_a = False[m
[32m+[m[32m                    args.train_b = True[m
[32m+[m[32m                else:[m
[32m+[m[32m                    args.train_a = True[m
[32m+[m[32m                    args.train_b = False[m
[32m+[m
         # debug list and rank:[m
         #args.block_ids_list[14] = [0,1][m
         #args.rank_list[14] = [3,1][m
[36m@@ -521,7 +535,7 @@[m [mdef update_global_model(args, global_model, local_updates, num_samples):[m
         if 'lora_B' in k:[m
             model_full_rank = global_model[k].shape[0][m
             break[m
[31m-    if args.lora_max_rank >= model_full_rank:[m
[32m+[m[32m    if args.lora_max_rank > model_full_rank:[m
         raise ValueError(f"lora_max_rank: {args.lora_max_rank} needs to be smaller than the model full rank {model_full_rank}")[m
 [m
     ### run svd[m
[36m@@ -804,7 +818,8 @@[m [mdef get_rank_list(args, layer_list, fim, id):[m
     sorted_layer_list = sorted(layer_list)[m
     selected_layer_fim = [fim[x] for x in sorted_layer_list][m
     # reserve 1-rank for each selected block[m
[31m-    rank_budget = getattr(args, 'var_rank_group'+str(id)+'_lora') - len(layer_list)[m
[32m+[m[32m    common_rank = 10[m
[32m+[m[32m    rank_budget = getattr(args, 'var_rank_group'+str(id)+'_lora') - common_rank*len(layer_list)[m
     normalized_selected_layer_fim = [x/sum(selected_layer_fim) for x in selected_layer_fim][m
     rank_list = [int(x*rank_budget) for x in normalized_selected_layer_fim][m
     left_over = rank_budget - sum(rank_list)[m
[36m@@ -812,10 +827,10 @@[m [mdef get_rank_list(args, layer_list, fim, id):[m
     rank_list[max_index] += left_over[m
 [m
     # add back the reserved rank for each block. Cap the rank assignment to the full rank setting.[m
[31m-    final_rank_list = [min(args.lora_max_rank,x + 1) for x in rank_list][m
[32m+[m[32m    final_rank_list = [min(args.lora_max_rank,x + common_rank) for x in rank_list][m
     args.rank_list.append(final_rank_list)[m
 [m
[31m-    print(f'group {id}: rank_budget = {rank_budget}, fim = {selected_layer_fim}, rank_list = {final_rank_list} ')[m
[32m+[m[32m    print(f'group {id}: rank_budget = {rank_budget}, fim = {selected_layer_fim}, rank_list = {final_rank_list}, selected layer = {sorted_layer_list} ')[m
     #print(f'args.rank_list = {args.rank_list}')[m
 [m
 def get_observed_probability(cluster_labels):[m
[1mdiff --git a/algorithms/engine/ffm_fedavg_depthfl.py b/algorithms/engine/ffm_fedavg_depthfl.py[m
[1mindex 2879dce..14e32dc 100755[m
[1m--- a/algorithms/engine/ffm_fedavg_depthfl.py[m
[1m+++ b/algorithms/engine/ffm_fedavg_depthfl.py[m
[36m@@ -142,7 +142,8 @@[m [mdef ffm_fedavg_depthfl(args):[m
                     args.block_ids_list.append(getattr(args, 'heterogeneous_group'+str(id)+'_lora'))[m
 [m
             # for exclusive and straggler tuning, all the rank are the same as the max lora rank[m
[31m-            args.rank_list = [[args.lora_max_rank]*args.lora_layer]*args.num_users[m
[32m+[m[32m            print(f'args.block_ids_list {args.block_ids_list}')[m
[32m+[m[32m            args.rank_list = [][m
 [m
     best_test_acc = 0.0[m
     best_test_f1 = 0.0[m
[1mdiff --git a/algorithms/solver/global_aggregator.py b/algorithms/solver/global_aggregator.py[m
[1mindex 2ef022d..1e07533 100755[m
[1m--- a/algorithms/solver/global_aggregator.py[m
[1m+++ b/algorithms/solver/global_aggregator.py[m
[36m@@ -44,9 +44,6 @@[m [mdef average_lora_depthfl(args, global_model, loc_updates):[m
 [m
     print('############## global aggregation ####################')[m
     lora_str = 'lora'[m
[31m-    if args.only_train_b:[m
[31m-        lora_str = 'lora_B'[m
[31m-        print('Only train Lora_B')[m
     for k in global_model.keys():[m
         if lora_str in k or 'classifier' in k:[m
             for loc_update in loc_updates:[m
[36m@@ -75,10 +72,6 @@[m [mdef weighted_average_lora_depthfl(args, global_model, loc_updates, num_samples):[m
     model_weights_list = {}[m
 [m
     lora_str = 'lora'[m
[31m-    if args.only_train_b:[m
[31m-        lora_str = 'lora_B'[m
[31m-        print('Only train Lora_B')[m
[31m-[m
     for k in global_model.keys():[m
         if lora_str in k or 'classifier' in k: # classifier is not included[m
             for client_i, loc_update in enumerate(loc_updates):[m
[1mdiff --git a/algorithms/solver/local_solver.py b/algorithms/solver/local_solver.py[m
[1mindex a034d12..f3416eb 100755[m
[1m--- a/algorithms/solver/local_solver.py[m
[1m+++ b/algorithms/solver/local_solver.py[m
[36m@@ -52,14 +52,18 @@[m [mclass LocalUpdate(object):[m
         # only train the lora module.[m
         for name, param in model.named_parameters():[m
             if ('lora' in name and any(('layer.' + str(nd) + '.') in name for nd in args.block_ids_list[client_real_id])) or 'classifier' in name:[m
[31m-                param.requires_grad = True[m
[32m+[m[32m                if args.train_b and 'lora_B' in name:[m
[32m+[m[32m                    print('train matrix b')[m
[32m+[m[32m                    param.requires_grad = True[m
[32m+[m[41m                [m
[32m+[m[32m                if args.train_a and 'lora_A' in name:[m
[32m+[m[32m                    print('train matrix a')[m
[32m+[m[32m                    param.requires_grad = True[m
[32m+[m[41m                [m
[32m+[m[41m                [m
             else:[m
                 param.requires_grad = False[m
         [m
[31m-        if args.only_train_b:[m
[31m-            for name, param in model.named_parameters():[m
[31m-                if 'lora_A' in name:[m
[31m-                    param.requires_grad = False[m
 [m
         if args.enable_rank_var:[m
             # add register to truncate the rank if rank variation is enable[m
[36m@@ -78,7 +82,7 @@[m [mclass LocalUpdate(object):[m
                     return grad[m
                 return hook[m
 [m
[31m-            print(f'client {client_real_id} block_ids_list = {args.block_ids_list[client_real_id]}, rank_list = {args.rank_list[client_real_id]}')[m
[32m+[m[32m            print(f'client {client_real_id} block_ids_list = {args.block_ids_list[client_real_id]}, rank_list = {args.rank_list[client_real_id]}, rank_budget = {sum(args.rank_list[client_real_id])}')[m
             for name, param in model.named_parameters():[m
                 if 'lora' in name and param.requires_grad:[m
                     layer_id = int(re.findall(r"\d+", name)[0])[m
[36m@@ -100,7 +104,12 @@[m [mclass LocalUpdate(object):[m
         #        print(name) [m
 [m
         # Note: Have to set the weight_decay to zero otherwise 0 gradient part will still be updated.[m
[31m-        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.local_lr,weight_decay=0.0)[m
[32m+[m[32m        # weight declay is set to zero only for rank variation[m
[32m+[m[32m        weight_decay = 0;[m
[32m+[m[32m        if args.enable_rank_var:[m
[32m+[m[32m            weight_decay = 0.01[m
[32m+[m
[32m+[m[32m        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.local_lr,weight_decay=weight_decay)[m
         # # Prepare everything with our `accelerator`.[m
         model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, ldr_train)[m
         total_loss = [][m
[1mdiff --git a/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_iid.yaml b/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_iid.yaml[m
[1mindex d51d210..577e9fd 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_iid.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_iid.yaml[m
[36m@@ -50,10 +50,12 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[32m+[m[32m#aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml b/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml[m
[1mindex 44c9d01..c640b7b 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml[m
[36m@@ -50,10 +50,11 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[32m+[m[32m#aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_20_dir.yaml b/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_20_dir.yaml[m
[1mindex f1742a7..a3ac540 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_20_dir.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_20_dir.yaml[m
[36m@@ -50,10 +50,11 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[32m+[m[32m#aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_iid.yaml b/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_iid.yaml[m
[1mindex 63f4244..814ead6 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_iid.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_iid.yaml[m
[36m@@ -51,10 +51,11 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[32m+[m[32m#aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_10_dir.yaml b/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_10_dir.yaml[m
[1mindex e5aca08..1347313 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_10_dir.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_10_dir.yaml[m
[36m@@ -50,10 +50,11 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[32m+[m[32m#aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_20_dir.yaml b/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_20_dir.yaml[m
[1mindex 55d361f..d1f4267 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_20_dir.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_20_dir.yaml[m
[36m@@ -50,10 +50,11 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[32m+[m[32m#aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-iid.yaml b/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-iid.yaml[m
[1mindex 77085ac..28a78ad 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-iid.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-iid.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-10.yaml b/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-10.yaml[m
[1mindex 45a742b..964c8a4 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-10.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-10.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-20.yaml b/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-20.yaml[m
[1mindex 2dd20b0..4d20765 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-20.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/FFA-LoRA/FFA-LORA-image-b-only-noniid-20.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/FedIT/FedIT-iid.yaml b/config/experiments/cifar100_vit_lora/FedIT/FedIT-iid.yaml[m
[1mindex b0941b7..5f00417 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/FedIT/FedIT-iid.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/FedIT/FedIT-iid.yaml[m
[36m@@ -54,7 +54,8 @@[m [mlora_max_rank: 12 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_10_dir.yaml b/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_10_dir.yaml[m
[1mindex 55a53ed..7cf773a 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_10_dir.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_10_dir.yaml[m
[36m@@ -53,7 +53,8 @@[m [mlora_max_rank: 12 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_20_dir.yaml b/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_20_dir.yaml[m
[1mindex bb65b5c..08a2cbc 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_20_dir.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/FedIT/FedIT-noniid-pat_20_dir.yaml[m
[36m@@ -53,7 +53,8 @@[m [mlora_max_rank: 12 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-iid.yaml b/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-iid.yaml[m
[1mindex 586c443..cef341a 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-iid.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-iid.yaml[m
[36m@@ -52,14 +52,15 @@[m [mvar_rank_group0_lora: 144 #24 *6[m
 var_rank_group1_lora: 216[m
 var_rank_group2_lora: 288[m
 [m
[31m-lora_max_rank: 32 # 384 is full rank already for vit[m
[32m+[m[32mlora_max_rank: 384 # 384 is full rank already for vit[m
 [m
 ##### Weighted aggregation[m
 # 'weighted_average' or normal avg when comment out[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: true[m
[36m@@ -67,6 +68,9 @@[m [mapply_svd_aggregation: true[m
 ###### Whether to train classifier (default: false)[m
 train_classifier: false[m
 [m
[32m+[m[32m###### disable[m[41m [m
[32m+[m[32mwarm_start: true[m
[32m+[m
 # Dataset configure[m
 data_type: image[m
 dataset: cifar100[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-10.yaml b/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-10.yaml[m
[1mindex 2e8371c..dc7da71 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-10.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-10.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 32 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: true[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-20.yaml b/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-20.yaml[m
[1mindex 75d790b..bf39396 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-20.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-20.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 32 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: true[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/backup/image-rank-variation-only.yaml b/config/experiments/cifar100_vit_lora/backup/image-rank-variation-only.yaml[m
[1mindex 4f14e0f..f5562c3 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/backup/image-rank-variation-only.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/backup/image-rank-variation-only.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 32 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true false[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/backup/image-svd-b-only.yaml b/config/experiments/cifar100_vit_lora/backup/image-svd-b-only.yaml[m
[1mindex 931e817..f3fe50b 100644[m
[1m--- a/config/experiments/cifar100_vit_lora/backup/image-svd-b-only.yaml[m
[1m+++ b/config/experiments/cifar100_vit_lora/backup/image-svd-b-only.yaml[m
[36m@@ -59,7 +59,8 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: true[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml b/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml[m
[1mdeleted file mode 100644[m
[1mindex 999d369..0000000[m
[1m--- a/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml[m
[1m+++ /dev/null[m
[36m@@ -1,91 +0,0 @@[m
[31m-_base_ : ['../../base_ffm.yaml'][m
[31m-[m
[31m-############## base differential #############[m
[31m-#num_users: 100 # number of users: K[m
[31m-#num_selected_users: 10 # number of selected users: 100, for shakespeare, it is 10, femnist 20[m
[31m-#round: 500 # rounds of training[m
[31m-[m
[31m-num_users: 20 # number of users: K[m
[31m-num_selected_users: 5 # number of selected users: 100, for shakespeare, it is 10, femnist 20[m
[31m-round: 200[m
[31m-tau: 1 # 5[m
[31m-# TODO Liam: for dev, change later[m
[31m-#batch_size: 128 # local batch size[m
[31m-batch_size: 32 # local batch size[m
[31m-optimizer: adamw[m
[31m-local_lr: 0.001 # local learning rate default:0.005[m
[31m-local_momentum: 0.5 # SGD Momentum default 0.5[m
[31m-decay_weight: 0.95 # learning rate decay weight default 0.5[m
[31m-global_momentum: 0.9 # global momentum[m
[31m-# clip: 1 # clipping threshold[m
[31m-########### base differential ends ###########[m
[31m-[m
[31m-lr_scheduler_type: linear[m
[31m-lr_step_size: 1[m
[31m-optimizer_weight_decay: 0.0[m
[31m-[m
[31m-# gradient_accumulation_steps: 1[m
[31m-# checkpointing_steps: 1[m
[31m-# per_device_train_batch_size: 8[m
[31m-# weight_decay: 0.0[m
[31m-# num_warmup_steps: 0[m
[31m-# use_trained: False[m
[31m-[m
[31m-method: ffm_fedavg[m
[31m-attack: None[m
[31m-defend: None[m
[31m-# Model[m
[31m-model: google/vit-base-patch16-224-in21k[m
[31m-peft: lora[m
[31m-[m
[31m-model_heterogeneity: depthffm_fim[m
[31m-heterogeneous_group: [1/3, 1/3, 1/3][m
[31m-lora_layer: 12[m
[31m-heterogeneous_group0_lora: 6[m
[31m-heterogeneous_group1_lora: 9[m
[31m-heterogeneous_group2_lora: 12[m
[31m-layer_prob: [1/9, 1/9, 1/9, 2/27, 2/27, 1/27, 1/27, 1/27, 2/27, 1/9, 1/9, 1/9][m
[31m-[m
[31m-##### rank variation[m
[31m-enable_rank_var: false[m
[31m-var_rank_group0_lora: 144 #16 [m
[31m-var_rank_group1_lora: 216[m
[31m-var_rank_group2_lora: 288[m
[31m-[m
[31m-lora_max_rank: 24 # 384 is full rank already for vit[m
[31m-[m
[31m-##### Weighted aggregation[m
[31m-# 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[31m-[m
[31m-##### fixed A for training[m
[31m-only_train_b: false[m
[31m-[m
[31m-##### apply SVD for aggregation[m
[31m-apply_svd_aggregation: false[m
[31m-[m
[31m-###### Whether to train classifier (default: false)[m
[31m-train_classifier: false[m
[31m-[m
[31m-# Dataset configure[m
[31m-data_type: image[m
[31m-dataset: cifar100[m
[31m-iid: 1 # whether i.i.d or not[m
[31m-[m
[31m-#####################################[m
[31m-########## non iid type #############[m
[31m-#####################################[m
[31m-# noniid_type: pathological # pathological, dirichlet(#Samples)[m
[31m-########## pathological ########[m
[31m-# pat_num_cls: 8 # 3 for cifar10[m
[31m-########## dirichlet ###########[m
[31m-# dir_cls_alpha: 0.3 # 1[m
[31m-################################[m
[31m-##### Parition Distribution ####[m
[31m-################################[m
[31m-# partition_mode: uni # uni, dir[m
[31m-[m
[31m-#####################################[m
[31m-########### fim params ##############[m
[31m-fim_every_iter: 10[m
[31m-fim_prior_epoch: 10[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_10_dir-noprior-s50-e50.yaml b/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_10_dir-noprior-s50-e50.yaml[m
[1mdeleted file mode 100644[m
[1mindex 8a6d5f8..0000000[m
[1m--- a/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_10_dir-noprior-s50-e50.yaml[m
[1m+++ /dev/null[m
[36m@@ -1,86 +0,0 @@[m
[31m-_base_ : ['../../base_ffm.yaml'][m
[31m-[m
[31m-num_users: 20 # number of users: K[m
[31m-num_selected_users: 5 # number of selected users: 100, for shakespeare, it is 10, femnist 20[m
[31m-round: 200[m
[31m-tau: 1 # 5[m
[31m-# TODO Liam: for dev, change later[m
[31m-#batch_size: 128 # local batch size[m
[31m-batch_size: 32 # local batch size[m
[31m-optimizer: adamw[m
[31m-local_lr: 0.001 # local learning rate default:0.005[m
[31m-local_momentum: 0.5 # SGD Momentum default 0.5[m
[31m-decay_weight: 0.95 # learning rate decay weight default 0.5[m
[31m-global_momentum: 0.9 # global momentum[m
[31m-# clip: 1 # clipping threshold[m
[31m-########### base differential ends ###########[m
[31m-[m
[31m-lr_scheduler_type: linear[m
[31m-lr_step_size: 1[m
[31m-optimizer_weight_decay: 0.0[m
[31m-[m
[31m-# gradient_accumulation_steps: 1[m
[31m-# checkpointing_steps: 1[m
[31m-# per_device_train_batch_size: 8[m
[31m-# weight_decay: 0.0[m
[31m-# num_warmup_steps: 0[m
[31m-# use_trained: False[m
[31m-[m
[31m-method: ffm_fedavg[m
[31m-attack: None[m
[31m-defend: None[m
[31m-# Model[m
[31m-model: google/vit-base-patch16-224-in21k[m
[31m-peft: lora[m
[31m-[m
[31m-model_heterogeneity: depthffm_fim[m
[31m-heterogeneous_group: [1/3, 1/3, 1/3][m
[31m-lora_layer: 12[m
[31m-heterogeneous_group0_lora: 6[m
[31m-heterogeneous_group1_lora: 9[m
[31m-heterogeneous_group2_lora: 12[m
[31m-layer_prob: [1/9, 1/9, 1/9, 2/27, 2/27, 1/27, 1/27, 1/27, 2/27, 1/9, 1/9, 1/9][m
[31m-[m
[31m-##### rank variation[m
[31m-enable_rank_var: false[m
[31m-var_rank_group0_lora: 144 #16 [m
[31m-var_rank_group1_lora: 216[m
[31m-var_rank_group2_lora: 288[m
[31m-[m
[31m-lora_max_rank: 24 # 384 is full rank already for vit[m
[31m-[m
[31m-##### Weighted aggregation[m
[31m-# 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[31m-[m
[31m-##### fixed A for training[m
[31m-only_train_b: false[m
[31m-[m
[31m-##### apply SVD for aggregation[m
[31m-apply_svd_aggregation: false[m
[31m-[m
[31m-###### Whether to train classifier (default: false)[m
[31m-train_classifier: false[m
[31m-[m
[31m-# Dataset configure[m
[31m-data_type: image[m
[31m-dataset: cifar100[m
[31m-iid: 0 # whether i.i.d or not[m
[31m-[m
[31m-#####################################[m
[31m-########## non iid type #############[m
[31m-#####################################[m
[31m-noniid_type: pathological # pathological, dirichlet(#Samples)[m
[31m-########## pathological ########[m
[31m-pat_num_cls: 10 # 3 for cifar10[m
[31m-########## dirichlet ###########[m
[31m-# dir_cls_alpha: 0.3 # 1.0 0.3[m
[31m-################################[m
[31m-##### Parition Distribution ####[m
[31m-################################[m
[31m-partition_mode: dir # uni, dir[m
[31m-[m
[31m-#####################################[m
[31m-########### fim params ##############[m
[31m-fim_every_iter: 50[m
[31m-fim_prior_epoch: 50[m
\ No newline at end of file[m
[1mdiff --git a/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_20_dir-noprior-s50-e50.yaml b/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_20_dir-noprior-s50-e50.yaml[m
[1mdeleted file mode 100644[m
[1mindex e8402d0..0000000[m
[1m--- a/config/experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_20_dir-noprior-s50-e50.yaml[m
[1m+++ /dev/null[m
[36m@@ -1,85 +0,0 @@[m
[31m-_base_ : ['../../base_ffm.yaml'][m
[31m-[m
[31m-num_users: 20 # number of users: K[m
[31m-num_selected_users: 5 # number of selected users: 100, for shakespeare, it is 10, femnist 20[m
[31m-round: 200[m
[31m-tau: 1 # 5[m
[31m-# TODO Liam: for dev, change later[m
[31m-#batch_size: 128 # local batch size[m
[31m-batch_size: 32 # local batch size[m
[31m-optimizer: adamw[m
[31m-local_lr: 0.001 # local learning rate default:0.005[m
[31m-local_momentum: 0.5 # SGD Momentum default 0.5[m
[31m-decay_weight: 0.95 # learning rate decay weight default 0.5[m
[31m-global_momentum: 0.9 # global momentum[m
[31m-# clip: 1 # clipping threshold[m
[31m-########### base differential ends ###########[m
[31m-lr_scheduler_type: linear[m
[31m-lr_step_size: 1[m
[31m-optimizer_weight_decay: 0.0[m
[31m-[m
[31m-# gradient_accumulation_steps: 1[m
[31m-# checkpointing_steps: 1[m
[31m-# per_device_train_batch_size: 8[m
[31m-# weight_decay: 0.0[m
[31m-# num_warmup_steps: 0[m
[31m-# use_trained: False[m
[31m-[m
[31m-method: ffm_fedavg[m
[31m-attack: None[m
[31m-defend: None[m
[31m-# Model[m
[31m-model: google/vit-base-patch16-224-in21k[m
[31m-peft: lora[m
[31m-[m
[31m-model_heterogeneity: depthffm_fim[m
[31m-heterogeneous_group: [1/3, 1/3, 1/3][m
[31m-lora_layer: 12[m
[31m-heterogeneous_group0_lora: 6[m
[31m-heterogeneous_group1_lora: 9[m
[31m-heterogeneous_group2_lora: 12[m
[31m-layer_prob: [1/9, 1/9, 1/9, 2/27, 2/27, 1/27, 1/27, 1/27, 2/27, 1/9, 1/9, 1/9][m
[31m-[m
[31m-##### rank variation[m
[31m-enable_rank_var: false[m
[31m-var_rank_group0_lora: 144 #16 [m
[31m-var_rank_group1_lora: 216[m
[31m-var_rank_group2_lora: 288[m
[31m-[m
[31m-lora_max_rank: 24 # 384 is full rank already for vit[m
[31m-[m
[31m-##### Weighted aggregation[m
[31m-# 'weighted_average' or normal avg when comment out[m
[31m-aggregation: 'weighted_average'[m
[31m-[m
[31m-##### fixed A for training[m
[31m-only_train_b: false[m
[31m-[m
[31m-##### apply SVD for aggregation[m
[31m-apply_svd_aggregation: false[m
[31m-[m
[31m-###### Whether to train classifier (default: false)[m
[31m-train_classifier: false[m
[31m-[m
[31m-# Dataset configure[m
[31m-data_type: image[m
[31m-dataset: cifar100[m
[31m-iid: 0 # whether i.i.d or not[m
[31m-[m
[31m-#####################################[m
[31m-########## non iid type #############[m
[31m-#####################################[m
[31m-noniid_type: pathological # pathological, dirichlet(#Samples)[m
[31m-########## pathological ########[m
[31m-pat_num_cls: 20 # 3 for cifar10[m
[31m-########## dirichlet ###########[m
[31m-# dir_cls_alpha: 0.3 # 1.0 0.3[m
[31m-################################[m
[31m-##### Parition Distribution ####[m
[31m-################################[m
[31m-partition_mode: dir # uni, dir[m
[31m-[m
[31m-#####################################[m
[31m-########### fim params ##############[m
[31m-fim_every_iter: 50[m
[31m-fim_prior_epoch: 50[m
\ No newline at end of file[m
[1mdiff --git a/config/experiments/ledgar_bert_lora/depthffm_fim/proposed-approach.yaml b/config/experiments/ledgar_bert_lora/depthffm_fim/proposed-approach.yaml[m
[1mindex c5120fd..f7c26a1 100644[m
[1m--- a/config/experiments/ledgar_bert_lora/depthffm_fim/proposed-approach.yaml[m
[1m+++ b/config/experiments/ledgar_bert_lora/depthffm_fim/proposed-approach.yaml[m
[36m@@ -58,7 +58,8 @@[m [mlora_max_rank: 54 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: true[m
[32m+[m[32mtrain_a: false[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: true[m
[1mdiff --git a/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-0_0_12-bone_iid-noprior-s50-e50.yaml b/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-0_0_12-bone_iid-noprior-s50-e50.yaml[m
[1mindex 42d3738..17a54f2 100644[m
[1m--- a/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-0_0_12-bone_iid-noprior-s50-e50.yaml[m
[1m+++ b/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-0_0_12-bone_iid-noprior-s50-e50.yaml[m
[36m@@ -63,7 +63,8 @@[m [mlora_max_rank: 24 # 128 is full rank already for bert[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_6_6-bone_iid-noprior-s50-e50.yaml b/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_6_6-bone_iid-noprior-s50-e50.yaml[m
[1mindex f7b772f..2ad8426 100644[m
[1m--- a/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_6_6-bone_iid-noprior-s50-e50.yaml[m
[1m+++ b/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_6_6-bone_iid-noprior-s50-e50.yaml[m
[36m@@ -63,7 +63,8 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml b/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml[m
[1mindex 23f2c49..b2febcd 100644[m
[1m--- a/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml[m
[1m+++ b/config/experiments/ledgar_bert_lora/depthffm_fim/text_ledgar_bert_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml[m
[36m@@ -58,7 +58,8 @@[m [mlora_max_rank: 24 # 384 is full rank already for vit[m
 aggregation: 'weighted_average'[m
 [m
 ##### fixed A for training[m
[31m-only_train_b: false[m
[32m+[m[32mtrain_a: true[m
[32m+[m[32mtrain_b: true[m
 [m
 ##### apply SVD for aggregation[m
 apply_svd_aggregation: false[m
[1mdiff --git a/main.py b/main.py[m
[1mindex 0a5a22e..031dd7a 100644[m
[1m--- a/main.py[m
[1m+++ b/main.py[m
[36m@@ -24,6 +24,10 @@[m [mdef merge_config(config, args):[m
 [m
 if __name__ == '__main__':[m
     parser = argparse.ArgumentParser()[m
[32m+[m[32m    parser.add_argument('--warm_start',[m[41m                   [m
[32m+[m[32m                        type=bool,[m
[32m+[m[32m                        default=False,[m
[32m+[m[32m                        help="warm start")[m
     parser.add_argument('--gpu',[m
                         type=list,[m
                         default=0,[m
[36m@@ -74,9 +78,9 @@[m [mif __name__ == '__main__':[m
         [m
         if meta_args.accelerator.is_local_main_process:[m
             args.logger.info('############ Case '+ str(r) + ' ############', main_process_only=True)[m
[31m-        torch.manual_seed(args.seed+r)[m
[32m+[m[32m        torch.manual_seed(args.seed*r)[m
         # torch.cuda.manual_seed(args.seed+args.repeat) # avoid[m
[31m-        np.random.seed(args.seed+r)[m
[32m+[m[32m        np.random.seed(args.seed*r)[m
         torch.backends.cudnn.deterministic = True[m
         torch.backends.cudnn.benchmark = False[m
         [m
[1mdiff --git a/run-cifar100.sh b/run-cifar100.sh[m
[1mindex a534e88..aed4417 100644[m
[1m--- a/run-cifar100.sh[m
[1m+++ b/run-cifar100.sh[m
[36m@@ -2,12 +2,12 @@[m
 ## CIFAR100 ##[m
 ##############[m
 [m
[31m-NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-10.yaml'[m
[32m+[m[32m#NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/Ours/image-rank-var-b-noniid-10.yaml'[m
 # Similar for other datasets ....[m
 [m
 [m
 # Fed-HeLLo-Bone[m
[31m-NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_10_dir-noprior-s50-e50.yaml' # Fed-HELLo Bone[m
[32m+[m[32m#NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_10_dir-noprior-s50-e50.yaml' # Fed-HELLo Bone[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_iid-noprior-s50-e50.yaml' # Fed-HELLo Bone[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/depthffm_fim/image-rank-variation-only.yaml' # Fed-HELLo Bone[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/depthffm_fim/image_cifar100_vit_fedavg_depthffm_fim-6_9_12-bone_noniid-pat_20_dir-noprior-s50-e50.yaml' # Fed-HELLo Bone[m
[36m@@ -15,12 +15,12 @@[m [mNCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_[m
 [m
 [m
 # Straggler Learning[m
[31m-NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml'[m
[32m+[m[32m#NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml'[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_10_dir.yaml'[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_20_dir.yaml'[m
 [m
 # Exclusive Learning[m
[31m-NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/6_6_6/image_cifar100_vit_fedavg_depthfl-6_6_6_noniid-pat_10_dir.yaml'[m
[32m+[m[32mNCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/Ours/image-rank-var-b-iid.yaml'[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_10_dir.yaml'[m
 #NCCL_DEBUG=INFO TORCH_DISTRIBUTED_DETAIL=DEBUG accelerate launch --main_process_port 29505 main.py --config_name 'experiments/cifar100_vit_lora/0_0_12/image_cifar100_vit_fedavg_depthfl-0_0_12_noniid-pat_20_dir.yaml'[m
 [m
[1mdiff --git a/utils/fim_calculator.py b/utils/fim_calculator.py[m
[1mindex f7a07a5..8b5eaa4 100644[m
[1m--- a/utils/fim_calculator.py[m
[1m+++ b/utils/fim_calculator.py[m
[36m@@ -99,7 +99,7 @@[m [mclass FIMCalculator:[m
                 torch.autograd.backward(samples[idx], retain_graph=True)[m
                 for name, param in model.named_parameters():[m
                     if param.requires_grad and 'classifier' not in name:[m
[31m-                        fim[name] += (param.grad) # no gradient square here. ||.||_F in the next step do the square[m
[32m+[m[32m                        fim[name] += (param.grad * param.grad)[m
                         fim[name].detach_()[m
                 seen_no += 1[m
                 idx += 1[m
[36m@@ -139,7 +139,7 @@[m [mclass FIMCalculator:[m
             layer_name = int(re.search(r'\.layer\.(\d+)\.', param_name).group(1))[m
             # layer_name is the model index[m
             # combined lora_A lora_B gradient[m
[31m-            fim_diag_by_layer[layer_name] += torch.norm(param_fim_diag, p='fro').item()[m
[32m+[m[32m            fim_diag_by_layer[layer_name] += torch.norm(param_fim_diag, p='fro').pow(2).item()[m
         return fim_diag_by_layer[m
 [m
     @staticmethod[m
